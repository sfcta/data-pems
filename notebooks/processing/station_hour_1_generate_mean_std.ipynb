{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historic-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak hour and peak period volumes for typical weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vertical-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, gzip, shutil\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import numpy as np\n",
    "import holidays\n",
    "#from shapely.geometry import Point, LineString\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fleet-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIR = r'Q:\\Model Projects\\101_280\\data\\pems'\n",
    "OUTDIR = r'Q:\\Model Projects\\101_280\\data\\pems\\with_holidays'\n",
    "data_type = 'station_hour'\n",
    "district = 4\n",
    "ca_holidays = holidays.UnitedStates(state='CA')\n",
    "\n",
    "start_year, end_year = 2005, 2022\n",
    "threshold = 25\n",
    "typical_weekday = True\n",
    "include_holidays = True\n",
    "sf_only = False\n",
    "continuous_only = False\n",
    "is_update = False\n",
    "data_type = 'processed_station_hour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ranking-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir(base, year=2020, data_type='station_hour', district=4):\n",
    "    if data_type in ['station_hour','station_5min','station_meta']:\n",
    "        return os.path.join(base,'D{}_Data_{}\\{}'.format(district,year,data_type))\n",
    "    elif data_type == 'processed_station_hour':\n",
    "        return os.path.join(base,'pems','pems_station_hour_{}.h5'.format(year))\n",
    "    \n",
    "def get_columns(data_type, num_cols):\n",
    "    if data_type == 'station_meta':\n",
    "        columns = ['station','route','dir','district','county','city','state_postmile','abs_postmile','latitude','longitude',\n",
    "                   'length','type','lanes','name','user_id_1','user_id_2','user_id_3','user_id_4']\n",
    "    if data_type == 'station_hour':\n",
    "        columns = ['timestamp', 'station', 'district', 'route', 'dir', 'lane_type', 'station_length',\n",
    "                   'samples', 'obs_pct', 'total_flow', 'avg_occupancy', 'avg_speed',\n",
    "                   'delay_35','delay_40','delay_45','delay_50','delay_55','delay_60']\n",
    "        for i in range(0, int((num_cols - 18) / 3)):\n",
    "            columns += [f'lane_{i}_flow',\n",
    "                        f'lane_{i}_avg_occ',\n",
    "                        f'lane_{i}_avg_speed',\n",
    "                       ]\n",
    "    if data_type == 'station_5min':\n",
    "        columns = ['timestamp', 'station', 'district', 'route', 'dir', 'lane_type', 'station_length',\n",
    "                   'samples', 'obs_pct', 'total_flow', 'avg_occupancy', 'avg_speed']\n",
    "        for i in range(0, int((num_cols - 12) / 5)):\n",
    "            columns += [f'lane_{i}_samples',\n",
    "                        f'lane_{i}_flow',\n",
    "                        f'lane_{i}_avg_occ',\n",
    "                        f'lane_{i}_avg_speed',\n",
    "                        f'lane_{i}_avg_obs',\n",
    "                       ]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extended-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv(os.path.join(INDIR,'stable_locations.csv'), infer_datetime_format=True, parse_dates=['start_date','end_date'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "administrative-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['start_date'] = locations['start_date'].map(lambda x: x.date())\n",
    "locations['end_date'] = locations['end_date'].map(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "international-scholarship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects\\101_280\\data\\pems\\pems_station_hour_2005.h5\n",
      "removing stations with unknown location before: 1379145, after: 871514\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2006.h5\n",
      "removing stations with unknown location before: 1390234, after: 861155\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2007.h5\n",
      "removing stations with unknown location before: 1554620, after: 930782\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2008.h5\n",
      "removing stations with unknown location before: 1703117, after: 992256\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2009.h5\n",
      "removing stations with unknown location before: 1681720, after: 977793\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2010.h5\n",
      "removing stations with unknown location before: 2121676, after: 1383790\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2011.h5\n",
      "removing stations with unknown location before: 2067558, after: 1429346\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2012.h5\n",
      "removing stations with unknown location before: 1786928, after: 1240011\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2013.h5\n",
      "removing stations with unknown location before: 2541465, after: 1764134\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2014.h5\n",
      "removing stations with unknown location before: 2789637, after: 1842750\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2015.h5\n",
      "removing stations with unknown location before: 3050293, after: 1837367\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2016.h5\n",
      "removing stations with unknown location before: 4172905, after: 2735320\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2017.h5\n",
      "removing stations with unknown location before: 5021559, after: 3434832\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2018.h5\n",
      "removing stations with unknown location before: 4865762, after: 3355487\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2019.h5\n",
      "removing stations with unknown location before: 4921470, after: 3437811\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2020.h5\n",
      "removing stations with unknown location before: 4406234, after: 3091534\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2021.h5\n",
      "removing stations with unknown location before: 4028323, after: 2248968\n",
      "Projects\\101_280\\data\\pems\\pems_station_hour_2022.h5\n",
      "removing stations with unknown location before: 2069025, after: 0\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "if sf_only:\n",
    "    locations = locations.loc[locations['county'].eq(75)]\n",
    "    \n",
    "for year in np.arange(start_year, end_year+1):\n",
    "    f = os.path.join(INDIR,'pems_station_hour_{}.h5'.format(year))\n",
    "    print(f.split()[1])\n",
    "    df = pd.read_hdf(f)\n",
    "\n",
    "    groupby_cols = ['station','era','hour']\n",
    "    \n",
    "    if typical_weekday and include_holidays:\n",
    "        df = df.loc[df['month'].isin([3,4,5,9,10,11]) & df['day_of_week'].isin([1,2,3]) & ~df['is_holiday']]\n",
    "    else:\n",
    "        groupby_cols = groupby_cols[:-1] + ['month','day_of_week'] + groupby_cols[-1:]\n",
    "    \n",
    "    if include_holidays and not typical_weekday:\n",
    "        groupby_cols = groupby_cols[:-1] + ['is_holiday'] + groupby_cols[-1:]\n",
    "    else:\n",
    "        df = df.loc[~df['is_holiday']]\n",
    "        \n",
    "    df = df.loc[df['obs_pct'].ge(threshold)]\n",
    "    before = len(df)\n",
    "    df = pd.merge(locations, df, on='station', suffixes=['','_obs'])\n",
    "    df = df.loc[df['date'].between(df['start_date'], df['end_date'])]\n",
    "    after = len(df)\n",
    "    print('removing stations with unknown location before: {}, after: {}'.format(before, after))\n",
    "    \n",
    "    era = 'pre-covid'\n",
    "    if year >= 2020:\n",
    "        era = 'covid-{}'.format(year)\n",
    "        \n",
    "    df.insert(0,'era', era)\n",
    "    m = df[['station','era','hour','total_flow']]\n",
    "    dfs.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "center-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e20a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "era\n",
       "covid-2020     3091534\n",
       "covid-2021     2248968\n",
       "pre-covid     27094348\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('era').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "contained-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_flow'] = df['total_flow'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "encouraging-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df.groupby(['station','hour','era']).agg({'total_flow':['mean','std','count']})\n",
    "m.columns = ['mean','std','n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "strong-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_update:\n",
    "    import shutil\n",
    "    old = pd.read_csv(os.path.join(OUTDIR,'station_flow_mean_std_{}_{}.csv'.format(start_year, end_year)))\n",
    "    shutil.copy(os.path.join(OUTDIR,'station_flow_mean_std_{}_{}.csv'.format(start_year, end_year)),\n",
    "                os.path.join(OUTDIR,'station_flow_mean_std_{}_{}_old.csv'.format(start_year, end_year)))\n",
    "    old.set_index(['station','hour','era'], inplace=True)\n",
    "    old.update(m)\n",
    "    m = old.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "danish-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caroline-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.to_csv(os.path.join(OUTDIR,'station_flow_mean_std_{}_{}.csv'.format(start_year, end_year)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
